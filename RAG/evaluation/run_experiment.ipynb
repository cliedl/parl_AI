{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "DATABASE_DIR_MANIFESTOS = \"../../data/manifestos/chroma/openai\"\n",
    "DATABASE_DIR_DEBATES = \"../../data/debates/chroma/openai\"\n",
    "TEST_DATA_PATH = \"../../data/questions/eval_questions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we define the experimental run! This is important for the filenames, etc!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"3_retrieval_fetch_k\"\n",
    "experiment_run_name = \"3e_fetch_k_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from models.RAG import RAG\n",
    "from database.vector_database import VectorDatabase\n",
    "from models.embedding import ManifestoBertaEmbeddings\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response):\n",
    "    q = response[next(iter(response))][\"question\"]\n",
    "    c = {party: response[party][\"context\"] for party in response.keys()}\n",
    "    d = {\n",
    "        source: {party: response[party][\"docs\"][source]\n",
    "                 for party in response.keys()}\n",
    "        for source in response[list(response.keys())[0]][\"docs\"].keys()\n",
    "    }\n",
    "    a = {party: response[party][\"answer\"] for party in response.keys()}\n",
    "    response = {\"question\": q, \"context\": c, \"docs\": d, \"answer\": a}\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.generation import generate_chain\n",
    "\n",
    "# Select an embedding_model\n",
    "########################################################################\n",
    "# embedding_model = ManifestoBertaEmbeddings()\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "\n",
    "# Select an LLM\n",
    "#########################################################################\n",
    "LARGE_LANGUAGE_MODEL = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", max_tokens=300, temperature=0\n",
    ")\n",
    "\n",
    "# LARGE_LANGUAGE_MODEL = ChatMistralAI(\n",
    "#     name=\"open-mixtral-8x7b\", max_tokens=300, temperature=0\n",
    "# )\n",
    "\n",
    "# LARGE_LANGUAGE_MODEL = ChatAnthropic(\n",
    "#     model_name=\"claude-3-haiku-20240307\", max_tokens=300, temperature=0\n",
    "# )\n",
    "\n",
    "\n",
    "db_manifestos = VectorDatabase(\n",
    "    data_path=\"../data/manifestos\",\n",
    "    embedding_model=embedding_model,\n",
    "    database_directory=DATABASE_DIR_MANIFESTOS,\n",
    "    source_type=\"manifestos\",\n",
    ")\n",
    "\n",
    "db_debates = VectorDatabase(\n",
    "    data_path=\"../data/debates\",\n",
    "    embedding_model=embedding_model,\n",
    "    database_directory=DATABASE_DIR_DEBATES,\n",
    "    source_type=\"debates\",\n",
    ")\n",
    "\n",
    "chain = generate_chain(\n",
    "    [db_manifestos, db_debates],\n",
    "    llm=LARGE_LANGUAGE_MODEL,\n",
    "    language=\"Deutsch\",\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "\n",
    "# rag = RAG(\n",
    "#     [db_manifestos, db_debates], llm=LARGE_LANGUAGE_MODEL, k=3, language=\"Deutsch\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset with question, context, and answer for chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_dataset = True\n",
    "if recreate_dataset:\n",
    "    # Create dataset with question, context and answer\n",
    "\n",
    "    # Load all test questions\n",
    "    # df_test_simple = pd.read_csv(os.path.join(TEST_DATA_DIR, \"simple_questions.csv\"))\n",
    "    # df_test_complex = pd.read_csv(os.path.join(TEST_DATA_DIR, \"complex_questions.csv\"))\n",
    "    df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    # Here we select, which parties we want to evaluate\n",
    "    parties = [\"afd\", \"spd\", \"linke\", \"gruene\", \"fdp\", \"cdu\"]\n",
    "\n",
    "    # Here we select the questions that should be evaluated\n",
    "    questions = df_test[\"question\"]\n",
    "\n",
    "    # Dictionary to save results\n",
    "    dataset_dict = {\n",
    "        party: {\"question\": [], \"contexts\": [], \"answer\": []} for party in parties\n",
    "    }\n",
    "\n",
    "    # Loop through all questions and get chain answer\n",
    "    for question in tqdm(questions):\n",
    "        print(question)\n",
    "        response = chain.invoke(question)\n",
    "        response = format_response(response)\n",
    "\n",
    "        # for pary in parties:\n",
    "        for party in parties:\n",
    "            contexts = [\n",
    "                doc.page_content for doc in response[\"docs\"][\"manifestos\"][party]\n",
    "            ]\n",
    "            answer = response[\"answer\"][party]\n",
    "\n",
    "            dataset_dict[party][\"question\"].append(question)\n",
    "            dataset_dict[party][\"contexts\"].append(contexts)\n",
    "            dataset_dict[party][\"answer\"].append(answer)\n",
    "\n",
    "    dataset = DatasetDict(\n",
    "        {party: Dataset.from_dict(dataset_dict[party]) for party in parties}\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    dataset.save_to_disk(\n",
    "        f\"Experiments/{experiment_name}/dataset_{experiment_run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(f\"Experiments/{experiment_name}/dataset_{experiment_run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Ragas and translate prompts to German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ragas\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_relevancy\n",
    "from evaluation import Evaluator\n",
    "\n",
    "# llm used for adaptation\n",
    "openai_model_adaption = ChatOpenAI(model_name=\"gpt-4\")\n",
    "ragas.adapt(\n",
    "    metrics=[faithfulness, answer_relevancy, context_relevancy],\n",
    "    language=\"german\",\n",
    "    llm=openai_model_adaption,\n",
    ")\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics with ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "\n",
    "# Configuration for ragas.evaluate\n",
    "run_config = RunConfig()\n",
    "run_config.max_retries = 3\n",
    "run_config.max_timeout = 60  # in s\n",
    "run_config.max_wait = 20  # in s\n",
    "\n",
    "num_questions = len(dataset[\"afd\"])\n",
    "questions = [dataset[\"fdp\"][i][\"question\"] for i in range(num_questions)]\n",
    "\n",
    "results_dict = {}\n",
    "# Define which parties should be evaluated\n",
    "parties = [\"afd\", \"spd\", \"linke\", \"gruene\", \"fdp\", \"cdu\"]\n",
    "\n",
    "# Define which metrics should be evaluated\n",
    "list_of_metrics = [faithfulness, answer_relevancy]\n",
    "\n",
    "for i in tqdm(range(num_questions)):\n",
    "    results_dict.update({questions[i]: {}})\n",
    "    print(f\"Question: {questions[i]}\")\n",
    "    for party in parties:\n",
    "        # print(f\"Evaluating party: {party}\")\n",
    "        # ragas_score = ragas.evaluate(\n",
    "        #     dataset[party].select([i]), metrics=list_of_metrics, run_config=run_config\n",
    "        # )\n",
    "        ragas_score = evaluator.context_relevancy(dataset[party].select([i]))\n",
    "\n",
    "        results_dict[questions[i]].update({party: ragas_score})\n",
    "    print(results_dict[questions[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dictionary\n",
    "flattened_data = {}\n",
    "for question, parties in results_dict.items():\n",
    "    for party, metrics in parties.items():\n",
    "        for metric, value in metrics.items():\n",
    "            flattened_data[(question, party, metric)] = value\n",
    "\n",
    "# Create a multi-index DataFrame\n",
    "index = pd.MultiIndex.from_tuples(flattened_data.keys())\n",
    "df = pd.Series(flattened_data, index=index).unstack().unstack()\n",
    "\n",
    "file_name = f\"Experiments/{experiment_name}/metrics_{experiment_run_name}.csv\"\n",
    "\n",
    "df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file saved correctly\n",
    "df = pd.read_csv(file_name, header=[0, 1], index_col=[0, 1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bundestag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
